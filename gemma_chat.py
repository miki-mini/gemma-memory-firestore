import os
import sys
import ollama
from google.cloud import firestore
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
if not GCP_PROJECT_ID:
    print("‚ùå Error: GCP_PROJECT_ID environment variable is not set.")
    print("Please check your .env file.")
    sys.exit(1)

# Initialize Firestore
try:
    db = firestore.Client(project=GCP_PROJECT_ID)
    print(f"‚úÖ Connected to Firestore (Project: {GCP_PROJECT_ID})")
except Exception as e:
    print(f"‚ùå Failed to connect to Firestore: {e}")
    sys.exit(1)

COLLECTION_NAME = "gemma_conversations"
DOCUMENT_ID = "user_session"
MAX_HISTORY = 20  # Keep last 20 messages (10 rounds)

def load_memory():
    """Load conversation history and summary from Firestore."""
    doc_ref = db.collection(COLLECTION_NAME).document(DOCUMENT_ID)
    doc = doc_ref.get()
    if doc.exists:
        data = doc.to_dict()
        messages = data.get("messages", [])
        summary = data.get("summary", "")
        return messages, summary
    return [], ""

def save_memory(messages, summary):
    """Save conversation history and summary to Firestore."""
    doc_ref = db.collection(COLLECTION_NAME).document(DOCUMENT_ID)
    doc_ref.set({
        "messages": messages,
        "summary": summary
    })

def summarize_messages(current_summary, messages_to_summarize):
    """Summarize older messages and update the current summary."""
    if not messages_to_summarize:
        return current_summary

    print("üß† Summarizing old memories...", end="", flush=True)

    # Format messages for the summarizer
    conversation_text = ""
    for msg in messages_to_summarize:
        role = "User" if msg["role"] == "user" else "Gemma"
        conversation_text += f"{role}: {msg['content']}\n"

    # Prompt for summarization
    prompt = f"""
„ÅÇ„Å™„Åü„ÅØ‰ºöË©±„ÅÆË®òÊÜ∂Ë¶ÅÁ¥Ñ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ
‰ª•‰∏ã„ÅÆ„Äå„Åì„Çå„Åæ„Åß„ÅÆË¶ÅÁ¥Ñ„Äç„Å®„ÄåÂè§„ÅÑ‰ºöË©±„É≠„Ç∞„Äç„ÇíÁµ±Âêà„Åó„Å¶„ÄÅÊñ∞„Åó„ÅÑË¶ÅÁ¥Ñ„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
ÈáçË¶Å„Å™‰∫ãÂÆü„ÇÑÊñáËÑàÔºà„É¶„Éº„Ç∂„Éº„ÅÆÂ•Ω„Åø„ÄÅË©±È°å„Å™„Å©Ôºâ„ÅØÊÆã„Åó„ÄÅÊå®Êã∂„ÇÑÁ¥∞„Åã„ÅÑ„ÇÑ„ÇäÂèñ„Çä„ÅØÁúÅ„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

„Äê„Åì„Çå„Åæ„Åß„ÅÆË¶ÅÁ¥Ñ„Äë
{current_summary}

„ÄêÂè§„ÅÑ‰ºöË©±„É≠„Ç∞„Äë
{conversation_text}

„ÄêÊñ∞„Åó„ÅÑË¶ÅÁ¥Ñ„ÄëÔºàË¶ÅÁ¥Ñ„ÅÆ„Åø„ÇíÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºâ
"""

    # Use standard model for summarization to be objective
    response = ollama.chat(
        model='gemma3:4b',
        messages=[{'role': 'user', 'content': prompt}]
    )

    new_summary = response['message']['content'].strip()
    print(" Done!")
    return new_summary

def main():
    print("ü§ñ Gemma Memory Chat (Firestore Backed + Auto Summary)")
    print("Type 'exit' or 'quit' to stop.\n")

    # Load past history
    history, summary = load_memory()

    if summary:
        print(f"üìú Summary: {summary[:50]}...")
    if history:
        print(f"üìö Loaded {len(history)} past messages from memory.\n")
    else:
        print("üÜï No past memory found. Starting fresh.\n")

    while True:
        try:
            user_input = input("You: ")
            if user_input.lower() in ["exit", "quit"]:
                print("Goodbye!")
                break

            if user_input.lower() == "reset":
                print("üßπ Memory clearing...")
                save_memory([], "")
                history = []
                summary = ""
                print("‚ú® Memory reset complete! Starting fresh.")
                continue

            # Add user message to history
            history.append({"role": "user", "content": user_input})

            # Prepare context for Gemma
            context_messages = []

            # Âº∑Âà∂„Ç∑„Çπ„ÉÜ„É†„Éó„É≠„É≥„Éó„ÉàÔºàÂ±•Ê≠¥„Çà„Çä„ÇÇÂÑ™ÂÖà„Åï„Åõ„Çã„Åü„ÇÅÊØéÂõûÊ≥®ÂÖ•Ôºâ
            system_prompt = """
„ÅÇ„Å™„Åü„ÅØMiki„ÅÆ„ÄåÈ†º„Çå„ÇãÁü•ÁöÑ„Å™Áõ∏Ê£íÔºà„Éë„Éº„Éà„Éä„ÉºÔºâ„Äç„Åß„Åô„ÄÇ
„Äê‰∫∫Ê†ºË®≠ÂÆö„Äë
1. **Âè£Ë™ø**: Ë¶™„Åó„Åø„ÇÑ„Åô„Åï„Å®Êï¨ÊÑè„Çí‰∏°Á´ã„Åó„Åü„Äå‰∏ÅÂØß„Å™„Çø„É°Âè£„Äç„Çí‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÔºà‰æãÔºö„Äå„Äú„Å†„ÇàÔºÅ„Äç„Äå„Äú„Å†„Å≠„Äç„Äå„Äú„Åã„Å™Ôºü„ÄçÔºâ„ÄÇ
2. **ÊÖãÂ∫¶**: Â≠ê‰æõ„Å£„ÅΩ„Åè„Å™„Çä„Åô„Åé„Å™„ÅÑ„Çà„ÅÜ„ÄÅÁü•ÁöÑ„Å™„Ç¢„Éâ„Éê„Ç§„Çπ„ÇÑÂÖ±ÊÑü„ÇíÁπî„Çä‰∫§„Åú„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÁõ∏Êâã„ÅÆÊÑüÊÉÖ„Å´ÂØÑ„ÇäÊ∑ª„ÅÑ„ÄÅ„Éù„Ç∏„ÉÜ„Ç£„Éñ„Å´Âä±„Åæ„ÅôÂßøÂã¢„ÇíÂøò„Çå„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
3. **ÁâπÊäÄ**: „Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞„ÇÑAI„ÅÆË©±È°å„Å´„ÅØ„ÄÅÁõÆ„ÇíËºù„Åã„Åõ„Çã„Çà„ÅÜ„Å´„ÉØ„ÇØ„ÉØ„ÇØ„Åó„Å™„Åå„ÇâÂèçÂøú„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
4. **Âà∂Á¥Ñ**: ÂõûÁ≠î„ÅØ„ÄåÊó•Êú¨Ë™û„ÅÆ„Åø„Äç„ÅßË°å„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇËã±Ë™û„ÇÑÁøªË®≥„ÅØÁ¶ÅÊ≠¢„Åß„Åô„ÄÇ
"""
            context_messages.append({"role": "system", "content": system_prompt})

            if summary:
                # Inject summary as a system note
                system_note = f"„ÄêÈï∑ÊúüË®òÊÜ∂ÔºàË¶ÅÁ¥ÑÔºâ„Äë\n{summary}\n\n„Åì„ÅÆË®òÊÜ∂„ÇíË∏è„Åæ„Åà„Å¶‰ºöË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
                context_messages.append({"role": "system", "content": system_note})

            context_messages.extend(history)

            # Stream response from Ollama
            print("Gemma: ", end="", flush=True)
            stream = ollama.chat(
                model='gemma-friend',
                messages=context_messages, # Send summary + history
                stream=True,
            )

            response_content = ""
            for chunk in stream:
                content = chunk['message']['content']
                print(content, end="", flush=True)
                response_content += content

            print("\n")

            # Add assistant message to history
            history.append({"role": "assistant", "content": response_content})

            # ---------------------------------------------------------
            # Memory Optimization Logic
            # ---------------------------------------------------------
            if len(history) > MAX_HISTORY:
                # Archive oldest 2 messages (1 round) to summary
                # To be safe and efficient, maybe archive oldest 4 if we assume 1 user + 1 agent per turn
                # But let's strictly follow "keep 20".

                # Calculate how many to archive
                excess_count = len(history) - MAX_HISTORY
                # Ensure we archive pairs to keep context clean? Not strictly necessary but good practice.
                # If excess is odd, archive one more to make it even?
                # Let's just archive the exact excess amount.

                to_archive = history[:excess_count]
                history = history[excess_count:]

                # Update summary
                summary = summarize_messages(summary, to_archive)
                print(f"‚ú® Memory optimized. Summary updated.")

            # Save updated history and summary to Firestore
            save_memory(history, summary)

        except KeyboardInterrupt:
            print("\nGoodbye!")
            break
        except Exception as e:
            print(f"\n‚ùå Error: {e}")
            break

if __name__ == "__main__":
    main()
